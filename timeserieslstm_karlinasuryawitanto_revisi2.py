# -*- coding: utf-8 -*-
"""TimeSeriesLSTM_KarlinaSuryaWitanto_Revisi2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QekVIn3nkgBirJWHSRXp5BY5Pm_N16fM

# **Proyek II : Membuat Model Machine Learning dengan Data Time Series - Dicoding x Kampus Merdeka**

# **DATA DIRI**

* Nama      : Karlina Surya Witanto
* ID        : M014V6051
* PT        : Universitas Udayana
* Email     : gabriella.linatan@gmail.com
* Email SIB : m014v6051@dicoding.org

# **1. Import Library & Import Dataset**

Sumber Dataset : https://www.kaggle.com/cornflake15/denpasarbalihistoricalweatherdata?select=openweatherdata-denpasar-1990-2020v0.1.csv
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from keras.layers import Dense, LSTM
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

from google.colab import drive
drive.mount('/content/drive/')

"""**Import Dataset dari Google Drive dan tampilkan dalam dataframe**"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/openweatherdata-denpasar-1990-2020v0.1.csv')
df.head(11)

"""# **2. Visualisasi Data - Grafik**"""

# Total data

df.shape

#menghitung jumlah data yang terbaru

df.weather_main.value_counts()

plt.figure(figsize = (12, 6))
sns.countplot(df.weather_main)

"""# **3. Mengecek Kolom yang Mengandung Variabel Kosong**"""

df.isnull().sum()

"""# **4. Drop Kolom yang Mengandung Variabel Kosong**"""

df=df.dropna(axis=1, how='any', thresh=None, subset=None, inplace=False)
df.head()

"""# **5. Drop Kolom yang Tidak Digunakan**"""

df.drop(columns=['lat', 'lon', 'timezone','weather_icon', 'city_name'],
                 inplace=True, axis=1)

"""# **6. Mengambil 15.000 Data**"""

df_train = df[1:12001]
df_val = df[12002:15002]

plt.figure(figsize=(15,5))
plt.plot(df_train.index, df_train['temp'])
plt.title('Temperature average',
          fontsize=20);
plt.plot(df_val.index, df_val['temp'])
plt.title('Temperature average',
          fontsize=20);

T_date = df_train['dt_iso']
T_temp  = df_train['temp'].values

V_date = df_val['dt_iso'].values
V_temp = df_val['temp'].values

T_temp_r=T_temp.reshape(-1, 1)
V_temp_r=V_temp.reshape(-1, 1)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    #series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

scaler = MinMaxScaler()
T_temp_r_s = scaler.fit_transform(T_temp_r)
V_temp_r_s = scaler.fit_transform(V_temp_r)

"""# **7. LSTM Model**"""

data_x_train = windowed_dataset(T_temp_r_s, window_size=60, batch_size=65, shuffle_buffer=1000)
data_x_test = windowed_dataset(V_temp_r_s, window_size=60, batch_size=65, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(128, return_sequences=True), 
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64, return_sequences=True),
    
  tf.keras.layers.Dense(60, activation="relu"),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))
#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-8)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

"""**Eksekusi Model LSTM**"""

history = model.fit(data_x_train ,epochs=65, validation_data=data_x_test, callbacks=[lr_schedule])

"""# **8. Membuat plot MAE dan loss**"""

import matplotlib.pyplot as plt

# plot of mae
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# plot of loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()